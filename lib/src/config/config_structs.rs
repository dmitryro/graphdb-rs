use std::collections::{HashSet, VecDeque,  BTreeMap};
use std::sync::Arc;
use derivative::Derivative;
use tokio::sync::{Mutex as TokioMutex, RwLock, mpsc };
use tokio::time::{timeout, Duration};
use tokio::net::TcpStream;
use tokio::task::JoinHandle;
use tokio::io::{AsyncReadExt, AsyncWriteExt};
use clap::{Args, Parser, Subcommand};
use uuid::Uuid;
use log::{debug, error, info, warn, trace};
use std::path::{Path, PathBuf};
use std::collections::{HashMap};
use serde::{de::DeserializeOwned, Deserialize, Serialize, Serializer, Deserializer};
use serde::de::{self, MapAccess, Visitor};
use serde_yaml2 as serde_yaml;
use std::time::{SystemTime, UNIX_EPOCH};
use serde_json::{Map, Value};
use tikv_client::{TransactionClient};
use crate::config::config_defaults::*;
use crate::config::config_constants::*;
use crate::config::config_serializers::*;
use crate::query_exec_engine::query_exec_engine::{QueryExecEngine};
use crate::commands::{Commands, CommandType, StatusArgs, RestartArgs, ReloadArgs, RestartAction,
                      ReloadAction, RestCliCommand, StatusAction, StorageAction, ShowAction, ShowArgs,
                      StartAction, StopAction, StopArgs, DaemonCliCommand, UseAction, SaveAction};
use crate::daemon_utils::{is_port_in_cluster_range, is_valid_cluster_range, parse_cluster_range};
pub use crate::storage_engine::storage_engine::{StorageEngineManager, GraphStorageEngine, GLOBAL_STORAGE_ENGINE_MANAGER};
pub use crate::storage_engine::sled_client::{ZmqSocketWrapper};
pub use crate::storage_engine::sled_wal_manager::{ SledWalManager };
pub use crate::storage_engine::rocksdb_client::{ZmqSocketWrapper as RocksdDBZmqSocketWrapper};
pub use crate::storage_engine::rocksdb_wal_manager::{ RocksDBWalManager };
pub use crate::indexing::{ IndexingService, backend::Document };
use models::{Vertex, Edge, Identifier, identifiers::SerializableUuid};
use models::errors::{GraphError, GraphResult};
use models::properties::PropertyValue;
use openraft_memstore::MemStore;
use openraft::{
    self, BasicNode, Entry, LogId, RaftLogReader, RaftSnapshotBuilder, RaftStorage, Snapshot,
    SnapshotMeta, RaftTypeConfig, StoredMembership, Vote, NodeId,
    // Traits and types required for network implementation
    network::{ RaftNetwork, RPCOption}, // ADD THIS import
    error::{RPCError, NetworkError, RaftError, Unreachable},
    raft::{AppendEntriesRequest, AppendEntriesResponse, InstallSnapshotRequest, InstallSnapshotResponse, VoteRequest, VoteResponse},
    error::InstallSnapshotError,
};
use std::error::Error; 
use bincode::{Encode, Decode};

pub type Service = Arc<TokioMutex<IndexingService>>; 

use rocksdb::{BoundColumnFamily, ColumnFamily, ColumnFamilyDescriptor, DB, Options, WriteOptions};

#[cfg(feature = "with-openraft-sled")]
use openraft_sled::SledRaftStorage;
use sled::{Config, Db, Tree};
use crate::daemon_registry::DaemonMetadata;
use zmq::{Context as ZmqContext, Socket };
// Raft type configuration
#[derive(Clone, Debug, Default, Serialize, Deserialize, Ord, PartialOrd, Eq, PartialEq, Copy)]
pub struct TypeConfig;

/// The type for a Raft node's unique ID.
pub type NodeIdType = u64;

// --- FIX: Define the required public type aliases at the module level ---
// This resolves the error: "cannot find type `DbArc` in this scope"
pub type DbArc = Arc<sled::Db>;
// The NodeId must be `u64` as per the `openraft` crate's definition.

// Define Raft application's request and response types.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum AppResponse {
    SetResponse(String),
    DeleteResponse,
}

impl RaftTypeConfig for TypeConfig {
    type D = AppRequest;
    type R = AppResponse;
    type NodeId = NodeIdType;
    type Node = NodeIdType; // Simplified to use NodeIdType directly
    type Entry = openraft::Entry<Self>;
    type SnapshotData = std::io::Cursor<Vec<u8>>;
    type AsyncRuntime = openraft::TokioRuntime;
    type Responder = openraft::raft::responder::OneshotResponder<Self>;
}

/*
#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct AppResponse {
    pub message: String,
}*/

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum RaftResponse {
    SetResponse(String),
    DeleteResponse,
}

/// A unified representation for objects stored in the graph (Vertices or Edges).
/// This is used to pass objects polymorphically to services like the Indexer.
#[derive(Clone, Debug, Serialize, Deserialize, Encode, Decode)]
pub enum GraphObject {
    Vertex(Vertex),
    Edge(Edge),
}

impl GraphObject {
    /// Helper to get the ID string for indexing purposes.
    pub fn id_string(&self) -> String {
        match self {
            GraphObject::Vertex(v) => v.id.0.to_string(),
            GraphObject::Edge(e) => e.id.0.to_string(),
        }
    }
}

/// Implements the conversion from GraphObject (Vertex or Edge) into the 
/// Document structure required by the indexing backend.
impl From<GraphObject> for Document {
    fn from(obj: GraphObject) -> Self {
        let (id_string, label_str, properties_map) = match obj {
            GraphObject::Vertex(v) => {
                // Assuming v.label is convertible to String (e.g., Identifier)
                let label_str = v.label.to_string(); 
                // Assuming v.properties is a HashMap<String, PropertyValue> and converting to BTreeMap
                let props: BTreeMap<String, PropertyValue> = v.properties.into_iter().collect();
                (v.id.0.to_string(), label_str, props)
            },
            GraphObject::Edge(e) => {
                // Assuming Edge already holds String label and BTreeMap properties
                (e.id.0.to_string(), e.label, e.properties)
            },
        };
        
        let mut fields: HashMap<String, String> = HashMap::new();

        // 1. Add the graph element label to the HashMap under a known key
        // This is CRITICAL for being able to query by node/edge type
        fields.insert("_label".to_string(), label_str); 

        // 2. Iterate and flatten all properties into the fields map
        for (key, value) in properties_map.into_iter() {
            // Flatten the complex PropertyValue enum into a simple String for indexing.
            // Using `format!("{:?}", value)` is a common way to serialize complex 
            // values into a searchable string representation if JSON serialization is not desired.
            let serialized_value = format!("{:?}", value); 
            fields.insert(key, serialized_value);
        }

        // 3. Construct the final Document structure using the correct fields
        Document {
            id: id_string,
            fields, // This matches the `pub fields: HashMap<String, String>` definition
        }
    }
}

/// Custom network implementation for Raft using TCP.
///
/// This struct is a placeholder for the actual network logic (serialization,
/// connection handling, etc.) but satisfies the RaftNetwork trait requirements.
/// It uses the types defined in `TypeConfig`.
#[derive(Debug, Clone)]
pub struct RaftTcpNetwork {}

// Define constants for network communication
const MAX_RETRIES: u8 = 3;
const BASE_DELAY_MS: u64 = 50;
const TIMEOUT_MS: u64 = 1000;

impl RaftNetwork<TypeConfig> for RaftTcpNetwork {
    /// Send an AppendEntries RPC.
    async fn append_entries(
        &mut self,
        request: AppendEntriesRequest<TypeConfig>,
        _option: RPCOption,
    ) -> Result<AppendEntriesResponse<u64>, RPCError<u64, u64, RaftError<u64>>> {
        // Extract target from the request - adjust based on actual request structure
        let target = request.vote.leader_id.node_id;
        let target_node = BasicNode { addr: format!("127.0.0.1:{}", target) };
        self.send_rpc("append_entries", target, target_node, request).await
    }

    /// Send an InstallSnapshot RPC.
    async fn install_snapshot(
        &mut self,
        request: InstallSnapshotRequest<TypeConfig>,
        _option: RPCOption,
    ) -> Result<InstallSnapshotResponse<u64>, RPCError<u64, u64, RaftError<u64, InstallSnapshotError>>> {
        let target = request.vote.leader_id.node_id;
        let target_node = BasicNode { addr: format!("127.0.0.1:{}", target) };
        self.send_rpc("install_snapshot", target, target_node, request).await
    }

    /// Send a Vote RPC.
    async fn vote(
        &mut self,
        request: VoteRequest<u64>,
        _option: RPCOption,
    ) -> Result<VoteResponse<u64>, RPCError<u64, u64, RaftError<u64>>> {
        let target = request.vote.leader_id.node_id;
        let target_node = BasicNode { addr: format!("127.0.0.1:{}", target) };
        self.send_rpc("vote", target, target_node, request).await
    }
}

impl RaftTcpNetwork {
    /// Generic RPC sender logic with retry and timeout.
    async fn send_rpc<REQ, RESP, ERR>(
        &mut self,
        rpc_name: &str,
        target: u64,
        _node: BasicNode,
        _request: REQ,
    ) -> Result<RESP, RPCError<u64, u64, ERR>>
    where
        REQ: serde::Serialize + std::fmt::Debug + Send + Sync + 'static,
        RESP: serde::de::DeserializeOwned + Send + Sync + 'static,
        ERR: Error + Send + Sync + 'static,
    {
        for attempt in 0..MAX_RETRIES {
            match timeout(Duration::from_millis(TIMEOUT_MS), async {
                info!("RAFT: Sending {} RPC to target {} (Attempt {})", rpc_name, target, attempt + 1);
                
                let dummy_response_str = match rpc_name {
                    "install_snapshot" => "{\"vote\": {\"leader_id\": {\"term\": 1, \"node_id\": 1}}}",
                    "vote" => "{\"vote\": {\"term\": 1, \"vote_granted\": true, \"last_log_id\": null}}",
                    _ => "{\"vote\": {\"term\": 1, \"vote_granted\": true}}",
                };

                let dummy_response = serde_json::from_str(dummy_response_str)
                    .map_err(|e| NetworkError::new(&std::io::Error::new(std::io::ErrorKind::InvalidData, e)))?;
                
                Ok::<RESP, NetworkError>(dummy_response)
            }).await {
                Ok(Ok(response)) => {
                    debug!("RAFT: Successfully received {} RPC response", rpc_name);
                    return Ok(response);
                }
                Ok(Err(e)) => {
                    error!("RAFT: Failed to send {} to target {}: {}", rpc_name, target, e);
                    return Err(RPCError::Network(e));
                }
                Err(timeout_err) => {
                    warn!("RAFT: Timeout sending {} to target {} on attempt {}. Retrying.", rpc_name, target, attempt + 1);
                    if attempt >= MAX_RETRIES - 1 {
                        error!("RAFT: Timeout sending {} to target {} after {} attempts.", rpc_name, target, MAX_RETRIES);
                        return Err(RPCError::Unreachable(Unreachable::new(&timeout_err)));
                    }
                    tokio::time::sleep(Duration::from_millis(BASE_DELAY_MS * (attempt as u64 + 1))).await;
                }
            }
        }
        unreachable!()
    }
}

/// Represents a plan for a graph query.
#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]
pub struct QueryPlan {
    pub query: String,
}

/// Represents the result of an executed graph query.
#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]
pub enum QueryResult {
    Success(String),
    Null,
}

/// Replication strategy for data operations
#[derive(Debug, Clone, Copy, Serialize, Deserialize)]
pub enum ReplicationStrategy {
    AllNodes,
    NNodes(usize),
    Raft,
}

/// Health status of a daemon node
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct NodeHealth {
    pub port: u16,
    pub is_healthy: bool,
    pub last_check: SystemTime,
    pub response_time_ms: u64,
    pub error_count: u32,
    pub request_count: u64,
}
/// Load balancer for routing requests across healthy nodes
#[derive(Derivative, Clone, Serialize, Deserialize)]
#[derivative(Debug, Default)]
pub struct LoadBalancer {
    #[serde(skip)]
    #[derivative(Debug = "ignore")]
    pub nodes: Arc<RwLock<HashMap<u16, NodeHealth>>>,

    #[serde(skip)]
    #[derivative(Debug = "ignore")]
    pub current_index: Arc<TokioMutex<usize>>,

    pub replication_factor: usize,

    #[serde(skip)]
    #[derivative(Debug = "ignore")]
    pub healthy_nodes: Arc<RwLock<VecDeque<NodeHealth>>>,

    #[serde(skip)]
    #[derivative(Debug = "ignore")]
    pub health_check_config: HealthCheckConfig,


    #[serde(skip)]
    #[derivative(Debug = "ignore")]
    pub next_node: usize,
}


#[derive(Clone)]
pub struct HealthCheckConfig {
    pub interval: Duration,
    pub connect_timeout: Duration,
    pub response_buffer_size: usize,
}

impl Default for HealthCheckConfig {
    fn default() -> Self {
        Self {
            interval: Duration::from_secs(10),
            connect_timeout: Duration::from_secs(2),
            response_buffer_size: 1024,
        }
    }
}

/// Storage engine types
#[derive(Debug, Clone, Copy, Serialize, Deserialize, PartialEq, Eq)]
#[serde(rename_all = "lowercase")]
pub enum StorageEngineType {
    Hybrid,
    Sled,
    RocksDB,
    TiKV,
    InMemory,
    Redis,
    PostgreSQL,
    MySQL,
}

pub struct MemStoreForTypeConfig {
    pub inner: Arc<MemStore>,
}

/// Engine configuration from YAML
#[derive(Debug, Deserialize)]
pub struct EngineConfig {
    pub storage: HashMap<String, Value>,
}

/// Struct to hold sled::Db and its path
#[derive(Debug, Clone)]
pub struct SledDbWithPath {
    pub db: Arc<sled::Db>,
    pub path: PathBuf,
    pub client: Option<(SledClient, Arc<TokioMutex<ZmqSocketWrapper>>)>,
}

/// Sled daemon configuration
#[derive(Debug)]
pub struct SledDaemon {
    pub port: u16,
    pub db_path: PathBuf,
    pub db: Arc<Db>,
    pub vertices: Tree,
    pub edges: Tree,
    pub kv_pairs: Tree,
    pub wal_manager: Arc<SledWalManager>,  // Add WAL manager
    pub running: Arc<TokioMutex<bool>>,
    pub zmq_thread: Option<std::thread::JoinHandle<()>>,  // Changed to store thread handle
    // ADDED: The initialized indexing service instance
    pub indexing_service: Service, 
    #[cfg(feature = "with-openraft-sled")]
    pub raft_storage: Arc<openraft_sled::SledRaftStorage<TypeConfig>>,
    #[cfg(feature = "with-openraft-sled")]
    pub node_id: u64,
}

/// Sled daemon pool
#[derive(Debug, Default)]
pub struct SledDaemonPool {
    pub daemons: HashMap<u16, Arc<SledDaemon>>,
    pub registry: Arc<RwLock<HashMap<u16, DaemonMetadata>>>,
    pub initialized: Arc<RwLock<bool>>,
    pub load_balancer: Arc<LoadBalancer>,
    pub use_raft_for_scale: bool, // Changed from use_raft
    // FIX 2: Wrapped the TokioMutex in an Arc to allow for safe, shared cloning.
    pub clients: Arc<TokioMutex<HashMap<u16, Arc<dyn GraphStorageEngine>>>>,
}

// FIX 3: Manual implementation of Clone.
// This implements Clone by calling .clone() on all fields.
// For Arc, this increases the reference count.
impl Clone for SledDaemonPool {
    fn clone(&self) -> Self {
        Self {
            daemons: self.daemons.clone(),
            registry: self.registry.clone(),
            initialized: self.initialized.clone(),
            load_balancer: self.load_balancer.clone(),
            use_raft_for_scale: self.use_raft_for_scale.clone(),
            // Arc::clone() is called here, which is what we need.
            clients: self.clients.clone(), 
        }
    }
}


/// Sled storage configuration
#[derive(Debug, Clone)]
pub struct SledStorage {
    pub pool: Arc<TokioMutex<SledDaemonPool>>,
    pub db: Arc<sled::Db>, // <-- Add this field
}

/// Struct to hold rocksdb::DB and its path
#[derive(Debug)]
pub struct RocksDBWithPath {
    pub db: Arc<DB>,
    pub path: PathBuf,
    pub client: Option<(RocksDBClient, Arc<TokioMutex<RocksdDBZmqSocketWrapper>>)>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum RaftCommand {
    SetKey { key: Vec<u8>, value: Vec<u8> },
    DeleteKey { key: Vec<u8> },
    CreateVertex(Vertex),
    UpdateVertex(Vertex),
    DeleteVertex(Uuid),
    CreateEdge(Edge),
    UpdateEdge(Edge),
    Set { key: String, value: String, cf: String },
    Insert { key: String, value: String, cf: String },
    Remove { key: String, cf: String },
    Delete { key: String, cf: String },
    DeleteEdge { outbound_id: Uuid, edge_type: Identifier, inbound_id: Uuid },
}


// The AppRequest enum wraps your RaftCommand. OpenRaft is generic over this
// type, allowing it to handle your specific application commands.
#[derive(Serialize, Deserialize, Debug, Clone)]
pub enum AppRequest {
    // The Command variant holds the actual application logic to be executed.
    Command(RaftCommand),
}

// Add an enum to track the client mode
#[derive(Debug, Clone, PartialEq)]
pub enum RocksDBClientMode {
    Direct,    // Direct database access (original behavior)
    ZMQ(u16),  // ZMQ communication with daemon on specified port
}

#[derive(Debug, Clone, PartialEq)]
pub enum SledClientMode {
    Direct,
    ZMQ(u16),
}

/// RocksDB Client
#[derive(Debug, Clone)]
pub struct RocksDBClient {
    pub inner: Option<Arc<TokioMutex<Arc<DB>>>>,
    pub db_path: Option<PathBuf>,
    pub is_running: Arc<TokioMutex<bool>>, 
    pub mode: Option<RocksDBClientMode>,
    pub zmq_socket: Option<Arc<TokioMutex<RocksdDBZmqSocketWrapper>>>,
}


#[derive(Clone)]
pub struct SledClient {
    pub context: Arc<ZmqContext>,  // ADD THIS - share context, not socket
    /// The actual Sled database handle (only present in local mode). 
    /// Protected by Arc and TokioMutex for thread-safe access.
    pub inner: Option<Arc<TokioMutex<Arc<Db>>>>,
    
    /// The path to the Sled database on disk.
    pub db_path: Option<PathBuf>,
    
    /// A thread-safe, mutable flag indicating if the client's associated service is running.
    pub is_running: Arc<TokioMutex<bool>>, 
    
    /// The mode of operation (Local or ZMQ-based).
    pub mode: Option<SledClientMode>,
    
    /// The ZMQ socket handle (only present in ZMQ mode), protected for concurrent use.
    pub zmq_socket: Option<Arc<TokioMutex<ZmqSocketWrapper>>>,
}

impl std::fmt::Debug for SledClient {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        f.debug_struct("SledClient")
            .field("context", &"<ZmqContext>") // Placeholder since Context doesn't implement Debug
            .field("inner", &self.inner)
            .field("db_path", &self.db_path)
            .field("is_running", &self.is_running)
            .field("mode", &self.mode)
            .field("zmq_socket", &self.zmq_socket)
            .finish()
    }
}

impl Drop for SledClient {
    fn drop(&mut self) {
        if let Some(socket_wrapper) = self.zmq_socket.take() {
            let socket_path = format!("/tmp/graphdb-{}.ipc", match self.mode {
                Some(SledClientMode::ZMQ(port)) => port,
                _ => DEFAULT_STORAGE_PORT,
            });
            let addr = format!("ipc://{}", socket_path);
            // Spawn a task to disconnect the socket asynchronously
            tokio::spawn(async move {
                let mut socket_guard = socket_wrapper.lock().await;
                if let Err(e) = socket_guard.disconnect(&addr) {
                    warn!("Failed to disconnect ZMQ socket from {} during drop: {}", addr, e);
                } else {
                    info!("Disconnected ZMQ socket from {} during SledClient drop", addr);
                }
            });
        }
    }
}
impl Drop for RocksDBClient {
    fn drop(&mut self) {
        // Use .take() to safely move the Arc out of the struct for the async task.
        if let Some(socket_wrapper) = self.zmq_socket.take() {
            // Determine the socket path based on the client mode
            let socket_path = format!("/tmp/graphdb-{}.ipc", match self.mode {
                Some(RocksDBClientMode::ZMQ(port)) => port,
                _ => DEFAULT_STORAGE_PORT,
            });
            let addr = format!("ipc://{}", socket_path);
            
            // Spawn a task to disconnect the socket asynchronously.
            // Note: ZMQ operations are generally synchronous, but we await the TokioMutex lock.
            tokio::spawn(async move {
                let mut socket_guard = socket_wrapper.lock().await;
                
                if let Err(e) = socket_guard.disconnect(&addr) {
                    warn!("Failed to disconnect ZMQ socket from {} during drop: {}", addr, e);
                } else {
                    // FIX: Changed the message to correctly reference RocksDBClient
                    info!("Disconnected ZMQ socket from {} during RocksDBClient drop", addr);
                }
            });
        }
    }
}

/// RocksDB Raft storage
#[derive(Clone)]
pub struct RocksDBRaftStorage {
    pub db: Arc<DB>,
    pub state_cf_name: String,
    pub log_cf_name: String,
    pub snapshot_cf_name: String,
    pub snapshot_meta_cf_name: String,
    pub state_cf: Arc<BoundColumnFamily<'static>>,
    pub log_cf: Arc<BoundColumnFamily<'static>>,
    pub snapshot_cf: Arc<BoundColumnFamily<'static>>,
    pub snapshot_meta_cf: Arc<BoundColumnFamily<'static>>,
    pub config: StorageConfig,
    pub client: Option<Arc<RocksDBClient>>,
}

#[derive(Clone)]
pub struct RocksDBDaemon<'a> {
    pub port: u16,
    pub db_path: PathBuf,
    pub db: Arc<DB>,
    pub kv_pairs: Arc<BoundColumnFamily<'a>>,
    pub vertices: Arc<BoundColumnFamily<'a>>,
    pub edges: Arc<BoundColumnFamily<'a>>,
    pub wal_manager: Arc<RocksDBWalManager>,
    pub running: Arc<TokioMutex<bool>>,
    pub shutdown_tx: mpsc::Sender<()>,
    pub zmq_context: Arc<ZmqContext>,
    //pub zmq_thread: Arc<TokioMutex<Option<JoinHandle<Result<(), GraphError>>>>>,
    pub zmq_thread: Arc<TokioMutex<Option<std::thread::JoinHandle<Result<(), GraphError>>>>>,
    // ADDED: The initialized indexing service instance
    pub indexing_service: Service, 
    #[cfg(feature = "with-openraft-rocksdb")]
    pub raft: Option<Arc<Raft<TypeConfig>>>,
    #[cfg(feature = "with-openraft-rocksdb")]
    pub raft_storage: Option<Arc<RocksDBRaftStorage>>,
    #[cfg(feature = "with-openraft-rocksdb")]
    pub node_id: u64,
        // === INTERNAL LAZY STATE ===
   // pub _lazy_db_state: Arc<TokioMutex<Option<(Arc<DB>, Arc<BoundColumnFamily<'a>>, Arc<BoundColumnFamily<'a>>, Arc<BoundColumnFamily<'a>>)>>>,

}

// IMPORTANT: Place this code block near the definition of the RocksDBDaemon struct.
// You are asserting thread safety, so ensure the underlying C library logic (RocksDB/ZMQ)
// handles concurrent access correctly when using clones/handles.
unsafe impl<'a> Send for RocksDBDaemon<'a> {}
unsafe impl<'a> Sync for RocksDBDaemon<'a> {}

// Manual implementation of Debug to handle non-Debug fields (ZmqContext, mpsc::Sender, RaftStorage).
impl<'a> std::fmt::Debug for RocksDBDaemon<'a> {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        let mut builder = f.debug_struct("RocksDBDaemon");
        
        builder.field("port", &self.port)
               .field("db_path", &self.db_path)
               // Print the memory address/pointer for Arc<DB> instead of the full database state
               .field("db", &format!("Arc<DB> @ {:?}", Arc::as_ptr(&self.db))); 
        // Explicitly omit non-Debug types to satisfy the compiler
        builder.field("running", &"Arc<TokioMutex<bool>> (omitted)");
        builder.field("shutdown_tx", &"mpsc::Sender<()> (omitted)");
        builder.field("zmq_context", &"Arc<ZmqContext> (omitted)");
        #[cfg(feature = "with-openraft-rocksdb")]
        {
            builder.field("raft_storage", &"Arc<RocksDBRaftStorage> (omitted)");
            builder.field("node_id", &self.node_id);
        }
        builder.finish()
    }
}

/// RocksDB daemon pool
// Note: 'Clone' attribute is removed because RwLock does not implement Clone.
// You will need to implement your own clone logic if needed, or rely on Arc<T>.
#[derive(Debug, Default, Clone)]
pub struct RocksDBDaemonPool {
    pub daemons: HashMap<u16, Arc<RocksDBDaemon<'static>>>,
    pub registry: Arc<RwLock<HashMap<u16, DaemonMetadata>>>,
    pub initialized: Arc<RwLock<bool>>,
    pub load_balancer: Arc<LoadBalancer>,
    pub use_raft_for_scale: bool,
    pub next_port: Arc<TokioMutex<u16>>,
    pub clients: Arc<TokioMutex<HashMap<u16, Arc<dyn GraphStorageEngine>>>>,
}

/// RocksDB storage configuration
#[derive(Debug, Clone)]
pub struct RocksDBStorage {
    pub use_raft_for_scale: bool,
    pub pool: Arc<TokioMutex<RocksDBDaemonPool>>,
    #[cfg(feature = "with-openraft-rocksdb")]
    pub raft: Option<openraft::Raft<TypeConfig>>,
    // Add the actual RocksDB instance
    pub db: Arc<DB>,
}

/// Engine type configuration
#[derive(Debug, Deserialize, Serialize, Clone)]
pub struct EngineTypeOnly {
    pub storage_engine_type: StorageEngineType,
}

/// Hybrid storage configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct HybridConfig {
    pub storage_engine_type: StorageEngineType,
    pub path: PathBuf,
    pub host: Option<String>,
    pub port: Option<u16>,
}

/// RocksDB configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct RocksDBConfig {
    pub storage_engine_type: StorageEngineType,
    pub path: PathBuf,
    pub host: Option<String>,
    pub port: Option<u16>,
    pub temporary: bool,
    pub use_compression: bool,
    pub use_raft_for_scale: bool,
    pub cache_capacity: Option<u64>,
    pub max_background_jobs: Option<u16>,
}

/// Sled configuration
#[derive(Debug, Clone, Deserialize, Serialize, PartialEq, Eq)]
#[serde(default)]
pub struct SledConfig {
    pub path: PathBuf,
    pub host: Option<String>,
    pub port: Option<u16>,
    pub temporary: bool,
    pub use_compression: bool,
    pub cache_capacity: Option<u64>,
    pub storage_engine_type: StorageEngineType,
}

impl Default for SledConfig {
    fn default() -> Self {
        SledConfig {
            host: Some(String::from("127.0.0.1")),
            port: Some(8049),
            path: PathBuf::from("/opt/graphdb/storage_data/sled"),
            temporary: false,
            use_compression: false,
            cache_capacity: None,
            storage_engine_type: StorageEngineType::Sled,
        }
    }
}

/// TiKV configuration
pub struct TikvStorage {
    pub client: Arc<TransactionClient>,
    pub config: TikvConfig, 
    pub running: TokioMutex<bool>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TikvConfig {
    pub storage_engine_type: StorageEngineType,
    pub path: PathBuf,
    pub host: Option<String>,
    pub port: Option<u16>,
    pub pd_endpoints: Option<String>,
    pub username: Option<String>,
    pub password: Option<String>,
}

/// Redis configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct RedisConfig {
    pub storage_engine_type: StorageEngineType,
    pub path: PathBuf,
    pub host: Option<String>,
    pub port: Option<u16>,
}

/// MySQL configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MySQLConfig {
    pub storage_engine_type: StorageEngineType,
    pub path: PathBuf,
    pub host: Option<String>,
    pub port: Option<u16>,
}

/// PostgreSQL configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PostgreSQLConfig {
    pub storage_engine_type: StorageEngineType,
    pub path: PathBuf,
    pub host: Option<String>,
    pub port: Option<u16>,
}

/// Generic DB configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct GenericConfig {
    #[serde(with = "storage_engine_type_serde")]
    pub storage_engine_type: StorageEngineType,
    #[serde(with = "option_path_buf_serde", default)]
    pub path: Option<PathBuf>,
    #[serde(default)]
    pub host: Option<String>,
    #[serde(default)]
    pub port: Option<u16>,
    #[serde(default)]
    pub username: Option<String>,
    #[serde(default)]
    pub password: Option<String>,
    #[serde(default)]
    pub database: Option<String>,
    #[serde(default)]
    pub name: Option<String>,
    #[serde(default)]
    pub current: Option<bool>,
     #[serde(default)]
   pub active: Option<bool>,
}

/// Raw storage configuration from YAML
#[derive(Debug, Deserialize, Serialize, Clone)]
#[serde(rename_all = "kebab-case")]
pub struct RawStorageConfig {
    pub config_root_directory: Option<PathBuf>,
    pub data_directory: Option<PathBuf>,
    pub log_directory: Option<PathBuf>,
    pub default_port: Option<u16>,
    pub cluster_range: Option<String>,
    pub max_disk_space_gb: Option<u64>,
    pub min_disk_space_gb: Option<u64>,
    pub use_raft_for_scale: Option<bool>,
    pub storage_engine_type: Option<StorageEngineType>,
    pub engine_specific_config: Option<HashMap<String, serde_json::Value>>,
    pub max_open_files: Option<u64>,
}

/// Inner storage configuration
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub struct StorageConfigInner {
    #[serde(with = "option_path_buf_serde", default)]
    pub path: Option<PathBuf>,
    #[serde(default)]
    pub host: Option<String>,
    #[serde(default)]
    pub port: Option<u16>,
    #[serde(default)]
    pub username: Option<String>,
    #[serde(default)]
    pub password: Option<String>,
    #[serde(default)]
    pub database: Option<String>,
    #[serde(default)]
    pub pd_endpoints: Option<String>,
    #[serde(default)]
    pub use_compression: bool,
    #[serde(default)]
    pub cache_capacity: Option<u64>,
    #[serde(default = "default_temporary")]
    pub temporary: bool,
    #[serde(default = "default_use_raft_for_scale")]
    pub use_raft_for_scale: bool,
}

fn default_use_raft_for_scale() -> bool {
    false // Matches StorageConfig default
}

fn default_temporary() -> bool {
    false
}

/// Selected storage configuration
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub struct SelectedStorageConfig {
    #[serde(with = "storage_engine_type_serde")]
    pub storage_engine_type: StorageEngineType,
    #[serde(flatten)]
    pub storage: StorageConfigInner,
}

/// Wrapper for selected storage configuration
#[derive(Debug, Deserialize, Serialize, Clone)]
pub struct SelectedStorageConfigWrapper {
    pub storage: SelectedStorageConfig,
}

/// Main daemon configuration
#[derive(Debug, Deserialize, Serialize, Clone)]
pub struct MainDaemonConfig {
    pub data_directory: String,
    pub log_directory: String,
    pub default_port: u16,
    pub cluster_range: String,
}

/// Wrapper for main daemon configuration
#[derive(Debug, Deserialize)]
pub struct MainConfigWrapper {
    pub main_daemon: MainDaemonConfig,
}

/// REST API configuration
#[derive(Debug, Deserialize, Serialize, Clone)]
pub struct RestApiConfig {
    pub data_directory: String,
    pub log_directory: String,
    pub default_port: u16,
    pub cluster_range: String,
}

/// Wrapper for REST API configuration
#[derive(Debug, Deserialize, Serialize)]
pub struct RestApiConfigWrapper {
    pub config_root_directory: String,
    pub rest_api: RestApiConfig,
}

/// Daemon YAML configuration
#[derive(Debug, Deserialize, Serialize, Clone)]
pub struct DaemonYamlConfig {
    pub config_root_directory: String,
    pub data_directory: String,
    pub log_directory: String,
    pub default_port: u16,
    pub cluster_range: String,
    pub max_connections: u32,
    pub max_open_files: u64,
    pub use_raft_for_scale: bool,
    pub log_level: String,
}

/// Storage configuration
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
#[serde(deny_unknown_fields)]
pub struct StorageConfig {
    #[serde(with = "option_path_buf_serde", default, alias = "config-root-directory")]
    pub config_root_directory: Option<PathBuf>,
    #[serde(with = "option_path_buf_serde", default = "default_data_directory", alias = "data-directory")]
    pub data_directory: Option<PathBuf>,
    #[serde(with = "option_path_buf_serde", default = "default_log_directory", alias = "log-directory")]
    pub log_directory: Option<PathBuf>,
    #[serde(default = "default_default_port", alias = "default-port")]
    pub default_port: u16,
    #[serde(with = "string_or_u16_non_option", default = "default_cluster_range", alias = "cluster-range")]
    pub cluster_range: String,
    #[serde(default = "default_max_disk_space_gb", alias = "max-disk-space-gb")]
    pub max_disk_space_gb: u64,
    #[serde(default = "default_min_disk_space_gb", alias = "min-disk-space-gb")]
    pub min_disk_space_gb: u64,
    #[serde(default = "default_use_raft_for_scale", alias = "use-raft-for-scale")]
    pub use_raft_for_scale: bool,
    #[serde(with = "storage_engine_type_serde", default = "default_storage_engine_type", alias = "storage-engine-type")]
    pub storage_engine_type: StorageEngineType,
    #[serde(default = "default_engine_specific_config")]
    pub engine_specific_config: Option<SelectedStorageConfig>,
    #[serde(default = "default_max_open_files", alias = "max-open-files")]
    pub max_open_files: u64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct StorageConfigWrapper {
    pub storage: StorageConfig,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EngineSpecificConfig {
    pub storage_engine_type: StorageEngineType,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub path: Option<PathBuf>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub host: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub port: Option<u16>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub database: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub username: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub password: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub pd_endpoints: Option<String>,
}

#[derive(Debug, Deserialize, Serialize)]
pub struct EngineSpecificConfigWrapper {
    storage: EngineSpecificConfig,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct TiKVConfigWrapper {
    pub storage: SpecificEngineFileConfig,
}

#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
#[serde(deny_unknown_fields)]
pub struct SpecificEngineFileConfig {
    #[serde(with = "storage_engine_type_serde", alias = "storage-engine-type")]
    pub storage_engine_type: StorageEngineType,
    #[serde(with = "option_path_buf_serde", default, alias = "path")]
    pub path: Option<PathBuf>,
    #[serde(default, alias = "host")]
    pub host: Option<String>,
    #[serde(default, alias = "port")]
    pub port: Option<u16>,
    #[serde(default, alias = "database")]
    pub database: Option<String>,
    #[serde(default, alias = "username")]
    pub username: Option<String>,
    #[serde(default, alias = "password")]
    pub password: Option<String>,
    #[serde(default, alias = "pd_endpoints")]
    pub pd_endpoints: Option<String>,
}

#[derive(Debug, Parser, Clone)]
#[clap(name = "GraphDB CLI", about = "Command line interface for GraphDB")]
pub struct CliConfig {
    #[clap(subcommand)]
    pub command: Commands,
    #[clap(
        long,
        env = "GRAPHDB_HTTP_TIMEOUT_SECONDS",
        default_value_t = DEFAULT_HTTP_TIMEOUT_SECONDS,
        help = "HTTP request timeout in seconds"
    )]
    pub http_timeout_seconds: u64,
    #[clap(
        long,
        env = "GRAPHDB_HTTP_RETRIES",
        default_value_t = DEFAULT_HTTP_RETRIES,
        help = "Number of HTTP request retries"
    )]
    pub http_retries: u32,
    #[clap(
        long,
        env = "GRAPHDB_GRPC_TIMEOUT_SECONDS",
        default_value_t = DEFAULT_GRPC_TIMEOUT_SECONDS,
        help = "gRPC request timeout in seconds"
    )]
    pub grpc_timeout_seconds: u64,
    #[clap(
        long,
        env = "GRAPHDB_GRPC_RETRIES",
        default_value_t = DEFAULT_GRPC_RETRIES,
        help = "Number of gRPC request retries"
    )]
    pub grpc_retries: u32,
    #[clap(long, help = "Port for the main GraphDB Daemon")]
    pub daemon_port: Option<u16>,
    #[clap(long, help = "Cluster range for the main GraphDB Daemon (e.g., '9001-9005')")]
    pub daemon_cluster: Option<String>,
    #[clap(long, help = "Port for the REST API")]
    pub rest_port: Option<u16>,
    #[clap(long, help = "Cluster range for the REST API")]
    pub rest_cluster: Option<String>,
    #[clap(long, help = "Port for the Storage Daemon (synonym for --port in storage commands)")]
    pub storage_port: Option<u16>,
    #[clap(long, help = "Cluster range for the Storage Daemon (synonym for --cluster in storage commands)")]
    pub storage_cluster: Option<String>,
    #[clap(long, help = "Path to the storage daemon configuration YAML")]
    pub storage_config_path: Option<PathBuf>,
    #[clap(long, help = "Path to the REST API configuration YAML")]
    pub rest_api_config_path: Option<PathBuf>,
    #[clap(long, help = "Path to the main daemon configuration YAML")]
    pub main_daemon_config_path: Option<PathBuf>,
    #[clap(long, help = "Root directory for configurations, if not using default paths")]
    pub config_root_directory: Option<PathBuf>,
    #[clap(long, short = 'c', help = "Run CLI in interactive mode")]
    pub cli: bool,
}

#[derive(Debug, Deserialize, Serialize, Clone)]
pub struct GlobalYamlConfig {
    pub storage: Option<StorageYamlConfig>,
    pub rest_api: Option<RestApiYamlConfig>,
    pub main_daemon: Option<MainDaemonYamlConfig>,
    pub config_root_directory: Option<PathBuf>,
}

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct StorageYamlConfig {
    pub default_port: u16,
    pub cluster_range: String,
}

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct RestApiYamlConfig {
    pub default_port: u16,
    pub cluster_range: String,
}

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct MainDaemonYamlConfig {
    pub default_port: u16,
    pub cluster_range: String,
}

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct AppConfig {
    pub version: Option<String>,
}

impl Default for AppConfig {
    fn default() -> Self {
        trace!("Creating default AppConfig");
        AppConfig { version: Some("0.1.0".to_string()) }
    }
}

#[derive(Debug, Serialize, Deserialize, Clone)]
#[serde(rename_all = "kebab-case")]
pub struct DeploymentConfig {
    #[serde(rename = "config-root-directory")]
    pub config_root_directory: Option<PathBuf>,
}

impl Default for DeploymentConfig {
    fn default() -> Self {
        trace!("Creating default DeploymentConfig");
        DeploymentConfig {
            config_root_directory: Some(PathBuf::from(DEFAULT_CONFIG_ROOT_DIRECTORY_STR)),
        }
    }
}

#[derive(Debug, Deserialize, Serialize, Default, Clone)]
pub struct LogConfig {}

#[derive(Debug, Deserialize, Serialize, Default, Clone)]
pub struct PathsConfig {}

#[derive(Debug, Deserialize, Serialize, Default, Clone)]
pub struct SecurityConfig {}

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct ServerConfig {
    pub port: Option<u16>,
    pub host: Option<String>,
    pub max_connections: u32,
}

impl Default for ServerConfig {
    fn default() -> Self {
        trace!("Creating default ServerConfig");
        ServerConfig {
            port: Some(DEFAULT_STORAGE_PORT),
            host: Some("127.0.0.1".to_string()),
            max_connections: 100,
        }
    }
}

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct RestConfig {
    pub port: Option<u16>,
    pub host: Option<String>,
}

impl Default for RestConfig {
    fn default() -> Self {
        trace!("Creating default RestConfig");
        RestConfig {
            port: Some(DEFAULT_REST_API_PORT),
            host: Some("127.0.0.1".to_string()),
        }
    }
}

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct DaemonConfig {
    pub port: Option<u16>,
    pub process_name: String,
    pub user: String,
    pub group: String,
    pub default_port: u16,
    pub cluster_range: String,
    pub max_connections: u32,
    pub max_open_files: u64,
    pub use_raft_for_scale: bool,
    pub log_level: String,
    #[serde(default)]
    pub log_directory: Option<String>,
}

impl Default for DaemonConfig {
    fn default() -> Self {
        trace!("Creating default DaemonConfig");
        DaemonConfig {
            port: None,
            process_name: "graphdb".to_string(),
            user: "graphdb".to_string(),
            group: "graphdb".to_string(),
            default_port: 8049,
            cluster_range: "8000-9000".to_string(),
            max_connections: 300,
            max_open_files: 100,
            use_raft_for_scale: false,
            log_level: "DEBUG".to_string(),
            log_directory: None,
        }
    }
}

#[derive(Debug, Deserialize, Serialize, Default, Clone)]
#[serde(rename_all = "snake_case")]
pub struct CliTomlStorageConfig {
    #[serde(default)]
    pub port: Option<u16>,
    #[serde(default)]
    pub default_port: Option<u16>,
    #[serde(with = "string_or_u16", default)]
    pub cluster_range: Option<String>,
    #[serde(default)]
    pub data_directory: Option<String>,
    #[serde(default)]
    pub config_root_directory: Option<PathBuf>,
    #[serde(default)]
    pub log_directory: Option<String>,
    #[serde(default)]
    pub max_disk_space_gb: Option<u64>,
    #[serde(default)]
    pub min_disk_space_gb: Option<u64>,
    #[serde(default)]
    pub use_raft_for_scale: Option<bool>,
    #[serde(with = "option_storage_engine_type_serde", default)]
    pub storage_engine_type: Option<StorageEngineType>,
    #[serde(default)]
    pub max_open_files: Option<u64>,
    #[serde(default)]
    pub config_file: Option<String>,
}

#[derive(Debug, Deserialize, Serialize, Clone)]
#[serde(rename_all = "snake_case")]
pub struct CliConfigToml {
    pub app: AppConfig,
    pub server: ServerConfig,
    pub rest: RestConfig,
    pub daemon: DaemonConfig,
    pub storage: CliTomlStorageConfig,
    pub deployment: DeploymentConfig,
    pub log: LogConfig,
    pub paths: PathsConfig,
    pub security: SecurityConfig,
    #[serde(default)]
    pub enable_plugins: bool,
}
